# ğŸŒ Guide de Voyage Intelligent : Pipeline Data Engineering End-to-End

## ğŸ“‹ Table des MatiÃ¨res
1. [Vue d'ensemble du projet](#-vue-densemble-du-projet)
2. [Architecture technique](#-architecture-technique)
3. [Structure des Fichiers](#-structure-des-fichiers)
4. [Installation et dÃ©ploiement](#-installation-et-dÃ©ploiement)
5. [Fonctionnement du SystÃ¨me](#-fonctionnement-du-systÃ¨me)
6. [DÃ©tails du Pipeline ETL](#-dÃ©tails-du-pipeline-etl)
7. [Configuration du Moteur de Recherche](#-configuration-du-moteur-de-recherche)
8. [DÃ©fis techniques et solutions](#-dÃ©fis-techniques-et-solutions)
9. [Maintenance et commandes utiles](#-maintenance-et-commandes-utiles)
10. [Conclusion](#-conclusion)

---

## ğŸ¯ Vue d'ensemble du projet

### ### Objectifs & Vision
L'objectif est de concevoir une plateforme capable d'agrÃ©ger, de traiter et de restituer des donnÃ©es de voyage de maniÃ¨re optimale. Ce projet dÃ©montre la mise en place d'un pipeline **ETL** (Extract, Transform, Load) moderne et automatisÃ©.

**Cas d'usage :** Un voyageur souhaite dÃ©couvrir les capitales europÃ©ennes tout en trouvant les meilleures adresses gastronomiques (**Guide Michelin**) Ã  proximitÃ©.

### ### FonctionnalitÃ©s clÃ©s
* âœ… **Scraping automatique** : RÃ©cupÃ©ration des donnÃ©es capitales (Routard) et restaurants (Michelin).
* âœ… **Stockage Hybride** : MongoDB pour la persistance et Elasticsearch pour la recherche "Fuzzy".
* âœ… **Recherche AvancÃ©e** : Moteur de recherche plein texte (par ville, type de cuisine, etc.).
* âœ… **Interface Web** : Visualisation dynamique sous Vue.js avec cartographie intÃ©grÃ©e.
* âœ… **Architecture Docker** : 5 micro-services isolÃ©s et orchestrÃ©s.

---

## ğŸ—ï¸ Architecture Technique

### ### Stack Technologique
| Composant | Technologie | Justification |
| :--- | :--- | :--- |
| **Scraping** | **Scrapy** | Gestion asynchrone permettant de scraper plusieurs villes en parallÃ¨le. |
| **Database** | **MongoDB** | FlexibilitÃ© du format JSON pour des donnÃ©es hÃ©tÃ©rogÃ¨nes. |
| **Search Engine** | **Elasticsearch** | Moteur de recherche plein texte performant. |
| **Backend** | **Flask** | API lÃ©gÃ¨re et robuste pour distribuer les donnÃ©es. |
| **Frontend** | **Vue.js** | Interface rÃ©active pour une expÃ©rience utilisateur fluide. |

---

## ğŸ“‚ Structure des Fichiers

L'organisation du projet suit une logique de sÃ©paration des prÃ©occupations par service :

```text
.
â”œâ”€â”€ backend/                # API Flask
â”‚   â”œâ”€â”€ app.py              # Points d'entrÃ©e de l'API
â”‚   â””â”€â”€ requirements.txt    # DÃ©pendances Python (Flask, PyMongo, Elasticsearch)
â”œâ”€â”€ frontend/               # Application Vue.js
â”‚   â”œâ”€â”€ src/                # Composants et logique Vue
â”‚   â””â”€â”€ package.json        # DÃ©pendances JS
â”œâ”€â”€ scraping/               # Projets Scrapy
â”‚   â”œâ”€â”€ michelin/           # Spider pour les restaurants
â”‚   â””â”€â”€ routard/            # Spider pour les capitales
â”œâ”€â”€ workers/                # Scripts d'importation et d'automatisation
â”‚   â”œâ”€â”€ import_data.py      # Script principal d'ingestion ETL
â”‚   â””â”€â”€ import_data.sh      # Script de contrÃ´le (Bash)
â”œâ”€â”€ docker-compose.yml      # Orchestration des services
â””â”€â”€ .gitignore              # Exclusion des fichiers inutiles
ğŸš€ Installation et DÃ©ploiement
### 1. PrÃ©paration de l'environnement
Ouvrez votre terminal (PowerShell ou Bash) et prÃ©parez le projet :

'Bash'

Bash

# Cloner le dÃ©pÃ´t
git clone [https://github.com/votre-username/DataEngineeringProject.git](https://github.com/votre-username/DataEngineeringProject.git)
cd DataEngineeringProject

# Configuration de Git pour Ã©viter les erreurs de fin de ligne (Windows)
git config core.autocrlf false
### 2. Lancement via Docker Compose
Une seule commande suffit pour construire les images et dÃ©marrer toute l'infrastructure :

'PowerShell'

PowerShell

# Construire et lancer les conteneurs
docker-compose up --build
Sortie attendue du terminal :

'Plaintext'

Plaintext

[+] Running 5/5
 âœ” Container mongodb_guide        Healthy
 âœ” Container elasticsearch_guide  Healthy
 âœ” Container flask_backend        Started
 âœ” Container vue_frontend         Started
 âœ” Container data_import_worker   Exited (0)  <-- Importation terminÃ©e avec succÃ¨s !
âš™ï¸ Fonctionnement du SystÃ¨me
### Flux de donnÃ©es nominal
Extraction : Les spiders Scrapy parcourent les sites cibles et gÃ©nÃ¨rent des fichiers JSON structurÃ©s.

Orchestration : Docker Compose lance les bases de donnÃ©es, puis le data_import_worker.

Ingestion : Le worker lit les JSON, nettoie les donnÃ©es et les injecte dans MongoDB et Elasticsearch via des scripts Python.

Consommation : L'API Flask interroge Elasticsearch pour les recherches textuelles et MongoDB pour les dÃ©tails complets, puis sert le Frontend Vue.js.

ğŸ“Š DÃ©tails du Pipeline ETL
### Extraction (Scrapy)
Le spider extrait les donnÃ©es et les formate en objets structurÃ©s avant exportation en JSON.

'Python'

Python

# Exemple d'extraction dans le spider Michelin
def parse_restaurant(self, response):
    yield {
        'nom': response.css('h1.restaurant-details__name::text').get().strip(),
        'cuisine': response.css('span.restaurant-details__cuisine::text').get(),
        'ville': response.meta['city'],
        'coordonnees': {
            'lat': response.css('meta[property="restaurant:location:latitude"]::attr(content)').get(),
            'lon': response.css('meta[property="restaurant:location:longitude"]::attr(content)').get()
        }
    }
ğŸ” Configuration du Moteur de Recherche
Elasticsearch est utilisÃ© pour fournir une recherche flexible ("fuzzy search"). Voici la configuration appliquÃ©e lors de l'ingestion :

### Indexation des donnÃ©es
Nous crÃ©ons un index restaurants avec un mapping spÃ©cifique pour optimiser les recherches sur le nom et le type de cuisine.

'Python'

Python

# Exemple de configuration de l'index dans le worker d'importation
mapping = {
    "mappings": {
        "properties": {
            "nom": {"type": "text", "analyzer": "french"},
            "cuisine": {"type": "keyword"},
            "ville": {"type": "keyword"}
        }
    }
}
es.indices.create(index='restaurants', body=mapping, ignore=400)
### Logique de Recherche
L'API Flask utilise des requÃªtes multi_match pour permettre Ã  l'utilisateur de trouver un restaurant mÃªme avec une faute de frappe.

ğŸ›¡ï¸ DÃ©fis Techniques et Solutions
### 1. Synchronisation des services (Race Condition)
ProblÃ¨me : Le script d'importation Ã©chouait car il tentait de se connecter Ã  MongoDB avant son dÃ©marrage complet. Solution : Utilisation d'une boucle d'attente active (wait-for-it) dans un script import_data.sh.

'Bash'

Bash

#!/bin/bash
echo "â³ Attente de la base de donnÃ©es MongoDB..."
while ! nc -z mongodb_guide 27017; do
  sleep 1
done
echo "âœ… MongoDB est prÃªt ! Lancement de l'importation..."
python import_data_guide_voyage.py
âš¡ Maintenance et Commandes Utiles
### VÃ©rifier les logs
'PowerShell'

PowerShell

docker-compose logs -f data_import_worker
### Explorer MongoDB
'PowerShell'

PowerShell

# Entrer dans le shell MongoDB
docker exec -it mongodb_guide mongosh

# Commandes utiles
use guide_db
db.restaurants.countDocuments()
db.restaurants.findOne({ville: "Paris"})
### RÃ©initialiser proprement le projet
'PowerShell'

PowerShell

# Supprimer les conteneurs et les volumes (efface les donnÃ©es)
docker-compose down -v
ğŸ Conclusion
Ce projet a permis de mettre en place une architecture Data Engineering complÃ¨te, allant de la collecte de donnÃ©es non structurÃ©es sur le web jusqu'Ã  leur mise Ã  disposition via une interface utilisateur moderne. L'utilisation de Docker garantit la reproductibilitÃ© de l'environnement, tandis que le couplage de MongoDB et Elasticsearch offre un Ã©quilibre parfait entre flexibilitÃ© de stockage et performance de recherche. Ce pipeline constitue une base solide pour l'ajout futur de nouvelles sources de donnÃ©es ou l'implÃ©mentation d'analyses prÃ©dictives sur les flux touristiques.

Projet rÃ©alisÃ© dans le cadre de l'Ã©valuation Data Engineering - 2026